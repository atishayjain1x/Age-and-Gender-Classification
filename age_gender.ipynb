{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "age_gender1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1QL_0Pgna1e5O_qRqGFiSabm5ZtkbztgG",
      "authorship_tag": "ABX9TyNfV5aspTAjMgVtvbpUxx2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atishayjain1x/Age-and-Gender-Classification/blob/main/age_gender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPATKmKbz5wS",
        "outputId": "9739842c-e38f-4e48-faf0-d9fc5d1629f3"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOVNY2x8Etz1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViDsGl-LGipz",
        "outputId": "98b9a7a9-1d30-488b-dbd5-76bab948fa5f"
      },
      "source": [
        "folder=\"drive/My Drive/ezyzip.zip\"\n",
        "!unzip \"/content/drive/My Drive/ezyzip.zip\" -d \"/content\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/ezyzip.zip\n",
            "replace /content/UTKFace/100_0_0_20170112213500903.jpg.chip.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqlcD7x6Yjcd"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten,BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, MaxPooling2D,Conv2D\n",
        "from tensorflow.keras.layers import Input,Activation,Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cHg8KSdHGhQ"
      },
      "source": [
        "p=\"/content/UTKFace\"\n",
        "import os\n",
        "files=os.listdir(p)\n",
        "\n",
        "def newlabel(age):\n",
        "\n",
        "    if 0<= age <= 2:\n",
        "        return 0\n",
        "    elif 3 <= age <= 8:\n",
        "        return 1\n",
        "    elif 9 <= age <= 14:\n",
        "        return 2\n",
        "    elif 15 <= age <= 20:\n",
        "        return 3\n",
        "    elif 21 <= age <= 27:\n",
        "        return 4\n",
        "    elif 28 <= age <= 35:\n",
        "        return 5\n",
        "    elif 36<=age<=45:\n",
        "        return 6\n",
        "    elif 46<=age<=59:\n",
        "        return 7\n",
        "    else:\n",
        "        return 8\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import cv2\n",
        "ages=[]\n",
        "genders=[]\n",
        "images=[]\n",
        "random.shuffle(files)\n",
        "\n",
        "for f in files:\n",
        "  age=int(f.split('_')[0])\n",
        "  gender=int(f.split('_')[1])\n",
        "  path=p+'/'+f\n",
        "  image=cv2.imread(path)\n",
        "  image =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "  image=cv2.resize(image,(48,48))\n",
        "  images.append(image)\n",
        "  ages.append(newlabel(age))\n",
        "  genders.append(gender)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt3w8Rwfov42"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "images_f=np.array(images)\n",
        "genders=np.array(genders)\n",
        "ages=np.array(ages)\n",
        "\n",
        "i=0\n",
        "\n",
        "p=[]\n",
        "q=[]\n",
        "labels=[]\n",
        "while (i<len(ages)):\n",
        "  l=to_categorical(ages[i],9)\n",
        "  m=to_categorical(genders[i],2)\n",
        "  p.append(l)\n",
        "  q.append(m)\n",
        "  i=i+1\n",
        "p=np.array(p).astype(np.float32) \n",
        "q=np.array(q).astype(np.float32)\n",
        "\n",
        "labels=[q,p]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TWrVegjpd2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee731baa-4ebb-4502-9549-9dcb0b92261f"
      },
      "source": [
        "for i in range(2):\n",
        "    imageset=images_f/255\n",
        "    imageset=np.array(imageset).astype(np.float32)\n",
        "    xtrain,xtest,ytrain,ytest=train_test_split(imageset,labels[i],test_size=0.25)\n",
        "    aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
        "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "    inputs=Input((48, 48, 3))\n",
        "    c1=Conv2D(32,kernel_size=3, activation=\"relu\")(inputs)\n",
        "    m1=MaxPooling2D(2,2)(c1)\n",
        "    c2=Conv2D(64,kernel_size=3, activation=\"relu\")(m1)\n",
        "    m2=MaxPooling2D(2,2)(c2)\n",
        "    m2=BatchNormalization()(m2)\n",
        "    c3=Conv2D(128,kernel_size=3, activation=\"relu\")(m2)\n",
        "    m3=MaxPooling2D(2,2)(c3)\n",
        "    c4=Conv2D(256,kernel_size=3, activation=\"relu\")(m3)\n",
        "    m4=MaxPooling2D(2,2)(c4)\n",
        "    m4=BatchNormalization()(m4)\n",
        "\n",
        "    f=(Flatten())(m4)\n",
        "    de2=Dense(units=256,activation='relu')(f)        \n",
        "    de3=Dense(units=64,activation='relu')(de2)\n",
        "    d2=BatchNormalization()(de3)\n",
        "    o1=Dense(2,activation=\"sigmoid\",name='sex_out')(d2)\n",
        "    o2=Dense(units=9,activation=\"softmax\",name='age_out')(d2)\n",
        "    out=[o1,o2]\n",
        "    ls=[\"binary_crossentropy\",\"categorical_crossentropy\"]\n",
        "    model = Model(inputs=[inputs], outputs=[out[i]])\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "    model.compile(loss=ls[i], optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
        "    model.fit(aug.flow(xtrain,ytrain,batch_size=250),validation_data=(xtest,ytest),epochs=60,callbacks=[callback])\n",
        "    if i==0:  \n",
        "      model.save(\"gender_detect_model\")\n",
        "    else:\n",
        "      model.save(\"age_detect_model\")\n",
        "    s=model.evaluate(xtest,ytest)\n",
        "    print(s)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "72/72 [==============================] - 14s 184ms/step - loss: 0.5467 - accuracy: 0.7282 - val_loss: 0.6653 - val_accuracy: 0.5240\n",
            "Epoch 2/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.4306 - accuracy: 0.7980 - val_loss: 0.6119 - val_accuracy: 0.6575\n",
            "Epoch 3/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.3874 - accuracy: 0.8215 - val_loss: 0.6625 - val_accuracy: 0.5473\n",
            "Epoch 4/60\n",
            "72/72 [==============================] - 13s 179ms/step - loss: 0.3586 - accuracy: 0.8405 - val_loss: 0.6577 - val_accuracy: 0.5569\n",
            "Epoch 5/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.3446 - accuracy: 0.8431 - val_loss: 0.6137 - val_accuracy: 0.6420\n",
            "Epoch 6/60\n",
            "72/72 [==============================] - 14s 188ms/step - loss: 0.3263 - accuracy: 0.8532 - val_loss: 0.4086 - val_accuracy: 0.7984\n",
            "Epoch 7/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 0.3172 - accuracy: 0.8554 - val_loss: 0.3498 - val_accuracy: 0.8382\n",
            "Epoch 8/60\n",
            "72/72 [==============================] - 14s 190ms/step - loss: 0.3221 - accuracy: 0.8560 - val_loss: 0.6019 - val_accuracy: 0.6648\n",
            "Epoch 9/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 0.2932 - accuracy: 0.8676 - val_loss: 0.3686 - val_accuracy: 0.8309\n",
            "Epoch 10/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 0.2922 - accuracy: 0.8692 - val_loss: 0.3113 - val_accuracy: 0.8650\n",
            "Epoch 11/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 0.2895 - accuracy: 0.8708 - val_loss: 0.3433 - val_accuracy: 0.8493\n",
            "Epoch 12/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.2831 - accuracy: 0.8745 - val_loss: 0.4202 - val_accuracy: 0.7894\n",
            "Epoch 13/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 0.2785 - accuracy: 0.8768 - val_loss: 0.2716 - val_accuracy: 0.8789\n",
            "Epoch 14/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 0.2694 - accuracy: 0.8806 - val_loss: 0.2842 - val_accuracy: 0.8799\n",
            "Epoch 15/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 0.2736 - accuracy: 0.8751 - val_loss: 0.2904 - val_accuracy: 0.8687\n",
            "Epoch 16/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.2616 - accuracy: 0.8847 - val_loss: 0.2994 - val_accuracy: 0.8746\n",
            "Epoch 17/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2608 - accuracy: 0.8848 - val_loss: 0.2524 - val_accuracy: 0.8947\n",
            "Epoch 18/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.2571 - accuracy: 0.8851 - val_loss: 0.2553 - val_accuracy: 0.8841\n",
            "Epoch 19/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2555 - accuracy: 0.8862 - val_loss: 0.4031 - val_accuracy: 0.8208\n",
            "Epoch 20/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.2577 - accuracy: 0.8853 - val_loss: 0.3200 - val_accuracy: 0.8622\n",
            "Epoch 21/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.2510 - accuracy: 0.8902 - val_loss: 0.2586 - val_accuracy: 0.8827\n",
            "Epoch 22/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2519 - accuracy: 0.8863 - val_loss: 0.2604 - val_accuracy: 0.8885\n",
            "Epoch 23/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.2444 - accuracy: 0.8931 - val_loss: 0.2503 - val_accuracy: 0.8863\n",
            "Epoch 24/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.2361 - accuracy: 0.8980 - val_loss: 0.3059 - val_accuracy: 0.8738\n",
            "Epoch 25/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.2373 - accuracy: 0.8979 - val_loss: 0.2339 - val_accuracy: 0.8988\n",
            "Epoch 26/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.2406 - accuracy: 0.8951 - val_loss: 0.2875 - val_accuracy: 0.8777\n",
            "Epoch 27/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.2357 - accuracy: 0.8994 - val_loss: 0.2409 - val_accuracy: 0.8954\n",
            "Epoch 28/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2321 - accuracy: 0.8994 - val_loss: 0.2470 - val_accuracy: 0.8912\n",
            "Epoch 29/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.2338 - accuracy: 0.9015 - val_loss: 0.2384 - val_accuracy: 0.8934\n",
            "Epoch 30/60\n",
            "72/72 [==============================] - 13s 181ms/step - loss: 0.2301 - accuracy: 0.9012 - val_loss: 0.2522 - val_accuracy: 0.8913\n",
            "Epoch 31/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2316 - accuracy: 0.8994 - val_loss: 0.2430 - val_accuracy: 0.8930\n",
            "Epoch 32/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2277 - accuracy: 0.8999 - val_loss: 0.2871 - val_accuracy: 0.8775\n",
            "Epoch 33/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2269 - accuracy: 0.8999 - val_loss: 0.2605 - val_accuracy: 0.8898\n",
            "Epoch 34/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.2249 - accuracy: 0.9057 - val_loss: 0.2849 - val_accuracy: 0.8777\n",
            "Epoch 35/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.2225 - accuracy: 0.9044 - val_loss: 0.2781 - val_accuracy: 0.8800\n",
            "Epoch 36/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2204 - accuracy: 0.9028 - val_loss: 0.2448 - val_accuracy: 0.8984\n",
            "Epoch 37/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.2174 - accuracy: 0.9066 - val_loss: 0.3077 - val_accuracy: 0.8849\n",
            "Epoch 38/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.2168 - accuracy: 0.9073 - val_loss: 0.3065 - val_accuracy: 0.8777\n",
            "Epoch 39/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2174 - accuracy: 0.9044 - val_loss: 0.2305 - val_accuracy: 0.8947\n",
            "Epoch 40/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.2093 - accuracy: 0.9108 - val_loss: 0.2686 - val_accuracy: 0.8859\n",
            "Epoch 41/60\n",
            "72/72 [==============================] - 13s 181ms/step - loss: 0.2109 - accuracy: 0.9109 - val_loss: 0.3092 - val_accuracy: 0.8682\n",
            "Epoch 42/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 0.2131 - accuracy: 0.9101 - val_loss: 0.3093 - val_accuracy: 0.8841\n",
            "Epoch 43/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 0.2173 - accuracy: 0.9066 - val_loss: 0.2905 - val_accuracy: 0.8704\n",
            "INFO:tensorflow:Assets written to: gender_detect_model/assets\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2905 - accuracy: 0.8704\n",
            "[0.2905431389808655, 0.8704234957695007]\n",
            "Epoch 1/60\n",
            "72/72 [==============================] - 15s 195ms/step - loss: 2.0129 - accuracy: 0.2595 - val_loss: 2.5777 - val_accuracy: 0.1218\n",
            "Epoch 2/60\n",
            "72/72 [==============================] - 14s 191ms/step - loss: 1.6523 - accuracy: 0.3636 - val_loss: 5.0930 - val_accuracy: 0.1218\n",
            "Epoch 3/60\n",
            "72/72 [==============================] - 14s 194ms/step - loss: 1.5141 - accuracy: 0.4043 - val_loss: 7.4363 - val_accuracy: 0.1218\n",
            "Epoch 4/60\n",
            "72/72 [==============================] - 14s 191ms/step - loss: 1.4378 - accuracy: 0.4282 - val_loss: 7.8443 - val_accuracy: 0.1218\n",
            "Epoch 5/60\n",
            "72/72 [==============================] - 14s 189ms/step - loss: 1.3793 - accuracy: 0.4486 - val_loss: 7.9784 - val_accuracy: 0.1218\n",
            "Epoch 6/60\n",
            "72/72 [==============================] - 14s 193ms/step - loss: 1.3592 - accuracy: 0.4485 - val_loss: 6.1338 - val_accuracy: 0.1222\n",
            "Epoch 7/60\n",
            "72/72 [==============================] - 14s 189ms/step - loss: 1.3199 - accuracy: 0.4658 - val_loss: 5.6898 - val_accuracy: 0.1220\n",
            "Epoch 8/60\n",
            "72/72 [==============================] - 13s 188ms/step - loss: 1.2909 - accuracy: 0.4751 - val_loss: 2.3575 - val_accuracy: 0.2372\n",
            "Epoch 9/60\n",
            "72/72 [==============================] - 14s 190ms/step - loss: 1.2744 - accuracy: 0.4832 - val_loss: 1.9185 - val_accuracy: 0.3042\n",
            "Epoch 10/60\n",
            "72/72 [==============================] - 14s 194ms/step - loss: 1.2723 - accuracy: 0.4797 - val_loss: 1.3094 - val_accuracy: 0.4552\n",
            "Epoch 11/60\n",
            "72/72 [==============================] - 14s 194ms/step - loss: 1.2704 - accuracy: 0.4821 - val_loss: 2.2447 - val_accuracy: 0.2770\n",
            "Epoch 12/60\n",
            "72/72 [==============================] - 14s 189ms/step - loss: 1.2316 - accuracy: 0.4944 - val_loss: 1.3927 - val_accuracy: 0.4145\n",
            "Epoch 13/60\n",
            "72/72 [==============================] - 13s 187ms/step - loss: 1.2146 - accuracy: 0.5019 - val_loss: 1.2893 - val_accuracy: 0.4755\n",
            "Epoch 14/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 1.2095 - accuracy: 0.5003 - val_loss: 1.6030 - val_accuracy: 0.3788\n",
            "Epoch 15/60\n",
            "72/72 [==============================] - 14s 188ms/step - loss: 1.1918 - accuracy: 0.5069 - val_loss: 1.7893 - val_accuracy: 0.3465\n",
            "Epoch 16/60\n",
            "72/72 [==============================] - 14s 187ms/step - loss: 1.1915 - accuracy: 0.5100 - val_loss: 1.2482 - val_accuracy: 0.4780\n",
            "Epoch 17/60\n",
            "72/72 [==============================] - 14s 188ms/step - loss: 1.1822 - accuracy: 0.5103 - val_loss: 1.5655 - val_accuracy: 0.4036\n",
            "Epoch 18/60\n",
            "72/72 [==============================] - 13s 187ms/step - loss: 1.1641 - accuracy: 0.5169 - val_loss: 1.5855 - val_accuracy: 0.4154\n",
            "Epoch 19/60\n",
            "72/72 [==============================] - 14s 188ms/step - loss: 1.1651 - accuracy: 0.5158 - val_loss: 1.2237 - val_accuracy: 0.4932\n",
            "Epoch 20/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.1457 - accuracy: 0.5265 - val_loss: 1.3477 - val_accuracy: 0.4485\n",
            "Epoch 21/60\n",
            "72/72 [==============================] - 13s 187ms/step - loss: 1.1505 - accuracy: 0.5214 - val_loss: 1.8703 - val_accuracy: 0.3484\n",
            "Epoch 22/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.1293 - accuracy: 0.5320 - val_loss: 1.2496 - val_accuracy: 0.4846\n",
            "Epoch 23/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 1.1314 - accuracy: 0.5318 - val_loss: 2.8163 - val_accuracy: 0.3390\n",
            "Epoch 24/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.1284 - accuracy: 0.5290 - val_loss: 1.3380 - val_accuracy: 0.4760\n",
            "Epoch 25/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.1178 - accuracy: 0.5323 - val_loss: 1.2887 - val_accuracy: 0.4636\n",
            "Epoch 26/60\n",
            "72/72 [==============================] - 14s 187ms/step - loss: 1.1118 - accuracy: 0.5358 - val_loss: 1.3916 - val_accuracy: 0.4510\n",
            "Epoch 27/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 1.1072 - accuracy: 0.5412 - val_loss: 1.2802 - val_accuracy: 0.4803\n",
            "Epoch 28/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 1.0962 - accuracy: 0.5432 - val_loss: 1.4573 - val_accuracy: 0.4301\n",
            "Epoch 29/60\n",
            "72/72 [==============================] - 14s 194ms/step - loss: 1.0966 - accuracy: 0.5465 - val_loss: 1.4511 - val_accuracy: 0.4679\n",
            "Epoch 30/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 1.0935 - accuracy: 0.5431 - val_loss: 1.5489 - val_accuracy: 0.4078\n",
            "Epoch 31/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 1.0832 - accuracy: 0.5493 - val_loss: 1.2606 - val_accuracy: 0.4901\n",
            "Epoch 32/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 1.0869 - accuracy: 0.5483 - val_loss: 1.1720 - val_accuracy: 0.5185\n",
            "Epoch 33/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 1.0746 - accuracy: 0.5540 - val_loss: 1.1660 - val_accuracy: 0.5003\n",
            "Epoch 34/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.0805 - accuracy: 0.5485 - val_loss: 1.1815 - val_accuracy: 0.5060\n",
            "Epoch 35/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.0823 - accuracy: 0.5490 - val_loss: 1.2507 - val_accuracy: 0.4827\n",
            "Epoch 36/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 1.0600 - accuracy: 0.5592 - val_loss: 1.4463 - val_accuracy: 0.4385\n",
            "Epoch 37/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 1.0679 - accuracy: 0.5544 - val_loss: 1.5005 - val_accuracy: 0.4086\n",
            "Epoch 38/60\n",
            "72/72 [==============================] - 13s 182ms/step - loss: 1.0640 - accuracy: 0.5555 - val_loss: 1.2978 - val_accuracy: 0.4604\n",
            "Epoch 39/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 1.0508 - accuracy: 0.5643 - val_loss: 1.2232 - val_accuracy: 0.5006\n",
            "Epoch 40/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.0484 - accuracy: 0.5609 - val_loss: 1.2164 - val_accuracy: 0.5043\n",
            "Epoch 41/60\n",
            "72/72 [==============================] - 13s 187ms/step - loss: 1.0462 - accuracy: 0.5634 - val_loss: 1.2786 - val_accuracy: 0.4952\n",
            "Epoch 42/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 1.0396 - accuracy: 0.5669 - val_loss: 1.5776 - val_accuracy: 0.4282\n",
            "Epoch 43/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 1.0428 - accuracy: 0.5694 - val_loss: 1.2086 - val_accuracy: 0.4888\n",
            "Epoch 44/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 1.0353 - accuracy: 0.5698 - val_loss: 1.1973 - val_accuracy: 0.5089\n",
            "Epoch 45/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.0263 - accuracy: 0.5683 - val_loss: 1.2151 - val_accuracy: 0.4964\n",
            "Epoch 46/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.0203 - accuracy: 0.5752 - val_loss: 1.2528 - val_accuracy: 0.4938\n",
            "Epoch 47/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 1.0220 - accuracy: 0.5729 - val_loss: 1.4306 - val_accuracy: 0.4355\n",
            "Epoch 48/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 1.0173 - accuracy: 0.5741 - val_loss: 1.5494 - val_accuracy: 0.4429\n",
            "Epoch 49/60\n",
            "72/72 [==============================] - 13s 183ms/step - loss: 1.0198 - accuracy: 0.5775 - val_loss: 1.8047 - val_accuracy: 0.3815\n",
            "Epoch 50/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 1.0144 - accuracy: 0.5784 - val_loss: 1.2983 - val_accuracy: 0.4844\n",
            "Epoch 51/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 1.0078 - accuracy: 0.5829 - val_loss: 1.2265 - val_accuracy: 0.4964\n",
            "Epoch 52/60\n",
            "72/72 [==============================] - 13s 184ms/step - loss: 0.9982 - accuracy: 0.5894 - val_loss: 1.2734 - val_accuracy: 0.4873\n",
            "Epoch 53/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 0.9970 - accuracy: 0.5824 - val_loss: 1.1895 - val_accuracy: 0.4940\n",
            "Epoch 54/60\n",
            "72/72 [==============================] - 14s 187ms/step - loss: 0.9913 - accuracy: 0.5885 - val_loss: 1.2996 - val_accuracy: 0.4721\n",
            "Epoch 55/60\n",
            "72/72 [==============================] - 14s 187ms/step - loss: 0.9773 - accuracy: 0.5989 - val_loss: 1.3126 - val_accuracy: 0.4706\n",
            "Epoch 56/60\n",
            "72/72 [==============================] - 14s 188ms/step - loss: 0.9812 - accuracy: 0.5910 - val_loss: 1.4362 - val_accuracy: 0.4660\n",
            "Epoch 57/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 0.9831 - accuracy: 0.5933 - val_loss: 1.2325 - val_accuracy: 0.5050\n",
            "Epoch 58/60\n",
            "72/72 [==============================] - 13s 187ms/step - loss: 0.9754 - accuracy: 0.5959 - val_loss: 1.2672 - val_accuracy: 0.5055\n",
            "Epoch 59/60\n",
            "72/72 [==============================] - 13s 186ms/step - loss: 0.9696 - accuracy: 0.5922 - val_loss: 1.2049 - val_accuracy: 0.5019\n",
            "Epoch 60/60\n",
            "72/72 [==============================] - 13s 185ms/step - loss: 0.9759 - accuracy: 0.5992 - val_loss: 1.2776 - val_accuracy: 0.4994\n",
            "INFO:tensorflow:Assets written to: age_detect_model/assets\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 1.2776 - accuracy: 0.4994\n",
            "[1.27764892578125, 0.4994094967842102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy3B0ul-LWlh"
      },
      "source": [
        "\n",
        "genderlabel=[\"Male\",\"Female\"]\n",
        "agelabel=[\"0-2\",\"3-8\",\"9-14\",\"15-20\",\"21-27\",\"28-35\",\"36-45\",\"46-59\",\"60+\"]\n",
        "from tensorflow.keras.models import load_model\n",
        "mod1=load_model(\"gender_detect_model\")\n",
        "mod2=load_model(\"age_detect_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BWSc-fKqbWH"
      },
      "source": [
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUApq0VtbigR"
      },
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  \n",
        "\n",
        "\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  img = js_to_image(data)\n",
        "  image =cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  image=cv2.resize(img,(48,48))\n",
        "  faces = image.astype(np.float32) / 255.0\n",
        "  faces=img_to_array(faces)\n",
        "  faces = np.expand_dims(faces, axis=0)\n",
        "  ge=mod1.predict(faces)\n",
        "  ag=mod2.predict(faces)\n",
        "  ge =np.argmax(ge)\n",
        "  ag=np.argmax(ag)\n",
        "  ge=genderlabel[ge]\n",
        "  ag=agelabel[ag]\n",
        "  lab=ag+' '+ge\n",
        "  cv2.putText(img,lab,(55,20),cv2.FONT_HERSHEY_SIMPLEX,0.4, (255,0,0),2)\n",
        "  cv2.imwrite(filename,img)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eRbbeqGLR9-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPsW8QzIbiqf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "fb25d4cb-a7a8-4939-88a7-8d17a85644b8"
      },
      "source": [
        "filename = take_photo('age_gender_image.jpg')\n",
        "print('Saved to {}'.format(filename))\n",
        "display(Image(filename))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saved to age_gender_image.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkKIfEaBhgwz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}